
There are broadly two specifications for what entropy is, one related to thermodynamics and one to information theory, that are related (debatably). It's also frequently tied with the ideas of chaos, order, uncertainty, and surprise.

Entropy is the the level of condensed, useful energy.

Entropy is the inverse of probability. The more probable something, the less uncertainty/less surprising it is.

The units of entropy depend on the base of the [logarithm](https://en.wikipedia.org/wiki/Logarithm "Logarithm"), which is usually 2 (i.e., the units are [bits](https://en.wikipedia.org/wiki/Bit "Bit")).

Entropy is the number of states in a system, consistent with some restraint" - [Steven Wolfram](https://youtu.be/cPfbGA_hNVo?si=XWiMSZC6jL5mNMoB&t=2026)

A distribution where all outcomes have similar probabilities (e.g., a uniform distribution) has high entropy because there is high uncertainty about which outcome will occur.

**High Entropy Means High Uncertainty**

• **High Entropy:** When entropy is high, it indicates that the outcomes are highly unpredictable. In the case of a uniform distribution, because all outcomes are equally likely, there is maximal uncertainty about which outcome will occur.

• **High Uncertainty:** Since each outcome has the same probability, there is no single outcome that we can predict with higher confidence over another. This equates to high uncertainty.

The high entropy of the universe indicates that, while we are almost certain that systems will be in disordered macrostates (low uncertainty about being disordered), there is still significant uncertainty about the exact microstate within that disordered macrostate. This is why entropy is considered a measure of disorder and uncertainty in thermodynamics.