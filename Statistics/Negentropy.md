It is the "opposite" of [[Entropy]], however it is a concept that is only used in the Information theory side of the discussion, not in thermodynamics.

For a given mean and variance, the Gaussian distribution has the maximal entropy. To calculate negentropy of a distribution, you would subtract the entropy of the distribution from the entropy of said Gaussian: $J(X) = H(X_{\text{Gaussian}}) - H(X)$, where $H$ is the function for computing entropy.

Negentropy is always non-negative, only equal to zero if the distribution is itself the Gaussian.